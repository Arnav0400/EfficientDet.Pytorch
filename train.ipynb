{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import Spine_dataset, get_augumentation, detection_collate\n",
    "from utils import EFFICIENTDET\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = None#'weights/checkpoint_efficientdet-d3_33.pth'\n",
    "network = 'efficientdet-d5'\n",
    "num_epochs = 40\n",
    "batch_size = 2\n",
    "num_worker = 4\n",
    "num_classes = 1\n",
    "device = [0]\n",
    "grad_accumulation_steps = 1\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "save_folder = '../weights/'\n",
    "image_root = 'boostnet_labeldata/data/'\n",
    "csv_root = 'boostnet_labeldata/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_device(device):\n",
    "    n_gpu_use = len(device)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu_use > 0 and n_gpu == 0:\n",
    "        print(\"Warning: There\\'s no GPU available on this machine, training will be performed on CPU.\")\n",
    "        n_gpu_use = 0\n",
    "    if n_gpu_use > n_gpu:\n",
    "        print(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available on this machine.\".format(\n",
    "            n_gpu_use, n_gpu))\n",
    "        n_gpu_use = n_gpu\n",
    "    list_ids = device\n",
    "    device = torch.device('cuda:{}'.format(\n",
    "        device[0]) if n_gpu_use > 0 else 'cpu')\n",
    "\n",
    "    return device, list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict(model):\n",
    "    if type(model) == torch.nn.DataParallel:\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "if(resume is not None):\n",
    "    resume_path = str(resume)\n",
    "    print(\"Loading checkpoint: {} ...\".format(resume_path))\n",
    "    checkpoint = torch.load(\n",
    "        resume, map_location=lambda storage, loc: storage)\n",
    "    num_classes = checkpoint['num_class']\n",
    "    network = checkpoint['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_train = pd.read_csv(csv_root+'training/landmarks.csv',header = None)\n",
    "filename_df_train = pd.read_csv(csv_root+'training/filenames.csv',header = None)\n",
    "boxes_df_train = pd.read_csv(csv_root+'training/train.csv')\n",
    "boxes_df_train.label = 0 # All boxes same class??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_test = pd.read_csv(csv_root+'test/landmarks.csv',header = None)\n",
    "filename_df_test = pd.read_csv(csv_root+'test/filenames.csv',header = None)\n",
    "boxes_df_test = pd.read_csv('test.csv')\n",
    "boxes_df_test.label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_train,corner_df_train,filename_df_train,transform=get_augumentation(phase = 'train',downsample = downsample),downsample = downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_test,corner_df_test,filename_df_test,transform=get_augumentation(phase = 'val',downsample = downsample),image_set='test', downsample = downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Spine_dataset.SPINEDetection_test(transform=get_augumentation(phase = 'test',downsample = downsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=None,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 3, 1536, 512]) torch.Size([2, 17, 5]) torch.Size([2, 17, 8])\n"
     ]
    }
   ],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    print(idx ,images.shape, annotations.shape, corners.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientDet(num_classes=num_classes,\n",
    "                     network=network,\n",
    "                     W_bifpn=EFFICIENTDET[network]['W_bifpn'],\n",
    "                     D_bifpn=EFFICIENTDET[network]['D_bifpn'],\n",
    "                     D_class=EFFICIENTDET[network]['D_class'],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(resume is not None):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "device, device_ids = prepare_device(device)\n",
    "model = model.to(device)\n",
    "if(len(device_ids) > 1):\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "if(resume is not None):\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=3, verbose=True)\n",
    "criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t start training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnav0400/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c21ae3c6884245973bf2ee73a5ed41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "best_loss = 100\n",
    "df = pd.DataFrame(np.zeros((num_epochs,6)),columns = [\"train_cls\",\"train_bbox_loss\",\"train_corner_loss\",\"val_cls\",\"val_bbox_loss\",\"val_corner_loss\"])\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"{} epoch: \\t start training....\".format(epoch))\n",
    "    start = time.time()\n",
    "    result = {}\n",
    "    total_loss = []\n",
    "    bbox_losses = []\n",
    "    cls_losses = []\n",
    "    corner_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    total_batches = len(train_dataloader)\n",
    "    tk0 = tqdm(train_dataloader, total=total_batches)\n",
    "    for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "        images = images.to(device)\n",
    "        annotations_bboxes = annotations_bboxes.to(device)\n",
    "        annotations_corners = annotations_corners.to(device)\n",
    "        classification, regression, corners, anchors = model(images)\n",
    "        classification_loss, regression_loss, corner_loss= criterion(\n",
    "            classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "        corner_loss = corner_loss.mean()\n",
    "        loss = classification_loss + regression_loss + corner_loss\n",
    "        if bool(loss == 0):\n",
    "            print('loss equal zero(0)')\n",
    "            continue\n",
    "        loss.backward()\n",
    "        if (idx+1) % grad_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_loss.append(loss.item())\n",
    "        corner_losses.append(corner_loss.item())\n",
    "        bbox_losses.append(regression_loss.item())\n",
    "        cls_losses.append(classification_loss.item())\n",
    "        tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "    result = {\n",
    "        'time': time.time() - start,\n",
    "        'loss': np.mean(total_loss),\n",
    "        'corner_loss': np.mean(corner_losses),\n",
    "        'bbox_loss': np.mean(bbox_losses),\n",
    "        'cls_loss': np.mean(cls_losses)\n",
    "    }\n",
    "    for key, value in result.items():\n",
    "        print('    {:15s}: {}'.format(str(key), value))\n",
    "    df.iloc[epoch,:3] = [np.mean(cls_losses),np.mean(bbox_losses),np.mean(corner_losses)] \n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        result = {}\n",
    "        total_loss = []\n",
    "        bbox_losses = []\n",
    "        cls_losses = []\n",
    "        corner_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        total_batches = len(test_dataloader)\n",
    "        tk0 = tqdm(val_dataloader, total=total_batches)\n",
    "        for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "            images = images.to(device)\n",
    "            annotations_bboxes = annotations_bboxes.to(device)\n",
    "            annotations_corners = annotations_corners.to(device)\n",
    "            classification, regression, corners, anchors = model(images)\n",
    "            classification_loss, regression_loss, corner_loss= criterion(\n",
    "                classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "            corner_loss = corner_loss.mean()\n",
    "            loss = classification_loss + regression_loss + corner_loss\n",
    "            if bool(loss == 0):\n",
    "                print('loss equal zero(0)')\n",
    "                continue\n",
    "            total_loss.append(loss.item())\n",
    "            corner_losses.append(corner_loss.item())\n",
    "            bbox_losses.append(regression_loss.item())\n",
    "            cls_losses.append(classification_loss.item())\n",
    "            tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "        result = {\n",
    "            'time': time.time() - start,\n",
    "            'loss': np.mean(total_loss),\n",
    "            'corner_loss': np.mean(corner_losses),\n",
    "            'bbox_loss': np.mean(bbox_losses),\n",
    "            'cls_loss': np.mean(cls_losses)\n",
    "        }\n",
    "        for key, value in result.items():\n",
    "            print('    {:15s}: {}'.format(str(key), value))\n",
    "            \n",
    "    scheduler.step(np.mean(total_loss))\n",
    "    df.iloc[epoch,3:] = [np.mean(cls_losses),np.mean(bbox_losses),np.mean(corner_losses)]\n",
    "    df.to_csv('../logs/d5-resize-corner-(1536,512).csv')\n",
    "    torch.cuda.empty_cache()\n",
    "    arch = type(model).__name__\n",
    "    state = {\n",
    "        'arch': arch,\n",
    "        'num_class': num_classes,\n",
    "        'network': network,\n",
    "        'state_dict': get_state_dict(model),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    if(best_loss>np.mean(corner_losses)):\n",
    "        best_loss = np.mean(corner_losses)\n",
    "        torch.save(state, '../weights/d5-resize-corner-(1536,512).pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(dataloader,split = 'test'):\n",
    "    if split=='test':\n",
    "        for idx, (images) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            image = images[0]\n",
    "            classification_score, category, boxes, corners = model(images)\n",
    "            break\n",
    "    else:\n",
    "        for idx, (images, annotations, corners) in enumerate(dataloader):\n",
    "            image = images[0]\n",
    "            images = images.to(device)\n",
    "#             classification_score, category, boxes, corners = model(images)\n",
    "            break\n",
    "    corners = corners.detach().cpu().numpy()\n",
    "    boxes = boxes.detach().cpu().numpy()\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    boxes = boxes.astype(np.int16)\n",
    "    for box in boxes:\n",
    "        start = (box[0],box[1])\n",
    "        end = (box[2],box[3])\n",
    "        image = cv2.rectangle(image, start, end, (255,0,0), 10)\n",
    "    plt.scatter(corners[:,:4],corners[:,4:])\n",
    "    plt.imshow(image.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    image = images[0]\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    corners = corners[0].detach().cpu().numpy()\n",
    "    corners = corners.astype(np.int16)\n",
    "    plt.scatter(corners[:,:4],corners[:,4:])\n",
    "    plt.imshow(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(train_dataloader,'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(dataloader = val_dataloader):\n",
    "    corners = []\n",
    "    clipped_corners = []\n",
    "    \n",
    "    # Predictions from model\n",
    "    for idx, (image, annotation, corner) in enumerate(val_dataloader):\n",
    "        image = image.to(device)\n",
    "        classification_score, category, pred_boxes, pred_corner = model(image)\n",
    "        classification_score = classification_score.detach().cpu().numpy()\n",
    "        pred_corner = pred_corner.detach().cpu().numpy()\n",
    "        if classification_score.shape[0]>17:\n",
    "            ind = np.argpartition(classification_score, -17)[-17:]\n",
    "            corners.append(pred_corner[ind])\n",
    "        else:\n",
    "            corners.append(pred_corner)\n",
    "    \n",
    "    #Making all predictions 17\n",
    "    for corner in corners:\n",
    "        if corner.shape[0]==17:\n",
    "            clipped_corners.append(corner)\n",
    "        else:\n",
    "            num_repeat = 17-corner.shape[0]\n",
    "            repeat = corner[-1].reshape(1,8)\n",
    "            for i in range(num_repeat):\n",
    "                corner = np.append(corner,repeat,axis=0)\n",
    "            clipped_corners.append(corner)\n",
    "    \n",
    "    clipped_corners = np.array(clipped_corners)\n",
    "    \n",
    "    #Reshaping and saving csv\n",
    "    val_landmarks = np.zeros((len(clipped_corners),136))\n",
    "    for i in range(len(clipped_corners)):\n",
    "        val_landmarks[i,:68] = clipped_corners[i,:,:4].reshape(1,68)/768\n",
    "        val_landmarks[i,68:] = clipped_corners[i,:,4:].reshape(1,68)/1408\n",
    "    \n",
    "    val_preds = pd.DataFrame(val_landmarks)\n",
    "    val_preds.to_csv('val_preds.csv',header = None,index = False)\n",
    "    return val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## find the SMAPE error\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.sum(np.abs(y_true-y_pred), axis=1)\n",
    "    denominator = np.sum(np.abs(y_true+y_pred), axis=1)\n",
    "    smape_val = np.mean(numerator/ denominator) * 100\n",
    "    return smape_val\n",
    "\n",
    "\n",
    "\n",
    "true_csv_path = \"boostnet_labeldata/labels/test/angles.csv\"\n",
    "pred_csv_path = \"boostnet_labeldata/labels/test/test_angles_polyfit.csv\"\n",
    "true_angles = pd.read_csv(true_csv_path, header=None).values\n",
    "pred_angles = pd.read_csv(pred_csv_path, header=None).values\n",
    "print (smape(true_angles, pred_angles))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
