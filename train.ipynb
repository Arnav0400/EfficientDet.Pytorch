{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import Spine_dataset, get_augumentation, detection_collate\n",
    "from utils import EFFICIENTDET\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = None#'weights/checkpoint_efficientdet-d3_33.pth'\n",
    "network = 'efficientdet-d5'\n",
    "num_epochs = 40\n",
    "batch_size = 2\n",
    "num_worker = 4\n",
    "num_classes = 1\n",
    "device = [0]\n",
    "grad_accumulation_steps = 1\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "save_folder = '../weights/'\n",
    "image_root = 'boostnet_labeldata/data/'\n",
    "csv_root = 'boostnet_labeldata/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_device(device):\n",
    "    n_gpu_use = len(device)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu_use > 0 and n_gpu == 0:\n",
    "        print(\"Warning: There\\'s no GPU available on this machine, training will be performed on CPU.\")\n",
    "        n_gpu_use = 0\n",
    "    if n_gpu_use > n_gpu:\n",
    "        print(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available on this machine.\".format(\n",
    "            n_gpu_use, n_gpu))\n",
    "        n_gpu_use = n_gpu\n",
    "    list_ids = device\n",
    "    device = torch.device('cuda:{}'.format(\n",
    "        device[0]) if n_gpu_use > 0 else 'cpu')\n",
    "\n",
    "    return device, list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict(model):\n",
    "    if type(model) == torch.nn.DataParallel:\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "if(resume is not None):\n",
    "    resume_path = str(resume)\n",
    "    print(\"Loading checkpoint: {} ...\".format(resume_path))\n",
    "    checkpoint = torch.load(\n",
    "        resume, map_location=lambda storage, loc: storage)\n",
    "    num_classes = checkpoint['num_class']\n",
    "    network = checkpoint['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_train = pd.read_csv(csv_root+'training/landmarks.csv',header = None)\n",
    "filename_df_train = pd.read_csv(csv_root+'training/filenames.csv',header = None)\n",
    "boxes_df_train = pd.read_csv(csv_root+'training/train.csv')\n",
    "boxes_df_train.label = 0 # All boxes same class??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_test = pd.read_csv(csv_root+'test/landmarks.csv',header = None)\n",
    "filename_df_test = pd.read_csv(csv_root+'test/filenames.csv',header = None)\n",
    "boxes_df_test = pd.read_csv('test.csv')\n",
    "boxes_df_test.label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_train,corner_df_train,filename_df_train,transform=get_augumentation(phase = 'train',downsample = downsample),downsample = downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_test,corner_df_test,filename_df_test,transform=get_augumentation(phase = 'val',downsample = downsample),image_set='test', downsample = downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Spine_dataset.SPINEDetection_test(transform=get_augumentation(phase = 'test',downsample = downsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=None,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([230., 302., 225., 296.,  34.,  49.,  72.,  96.]), array([220., 294., 211., 275.,  86., 109., 143., 169.]), array([209., 273., 189., 253., 153., 181., 202., 239.]), array([183., 250., 169., 234., 218., 252., 269., 300.]), array([160., 230., 145., 217., 287., 316., 346., 364.]), array([145., 215., 133., 205., 363., 377., 425., 430.]), array([135., 207., 135., 216., 446., 447., 513., 507.]), array([137., 216., 139., 221., 534., 517., 594., 580.]), array([141., 225., 154., 237., 620., 593., 684., 659.]), array([159., 240., 171., 251., 700., 672., 766., 741.]), array([176., 256., 186., 268., 788., 758., 858., 840.]), array([183., 270., 190., 288., 884., 860., 966., 934.]), array([ 196.,  293.,  203.,  316.,  995.,  958., 1058., 1027.]), array([ 212.,  317.,  220.,  326., 1091., 1057., 1161., 1128.]), array([ 222.,  328.,  227.,  337., 1188., 1153., 1265., 1242.]), array([ 234.,  358.,  235.,  360., 1290., 1268., 1372., 1349.]), array([ 235.,  363.,  244.,  364., 1393., 1385., 1465., 1466.])]\n",
      "[array([210., 294., 206., 301.,  29.,  34.,  78.,  79.]), array([210., 300., 202., 291.,  95.,  99., 144., 156.]), array([203., 286., 189., 285., 158., 169., 211., 224.]), array([190., 279., 179., 269., 224., 238., 282., 292.]), array([179., 265., 161., 257., 292., 306., 352., 370.]), array([156., 256., 148., 249., 364., 380., 428., 434.]), array([152., 251., 151., 252., 441., 443., 496., 496.]), array([147., 254., 152., 261., 516., 507., 586., 559.]), array([160., 261., 173., 275., 595., 570., 651., 626.]), array([180., 279., 200., 308., 670., 638., 731., 693.]), array([206., 314., 224., 335., 750., 710., 817., 776.]), array([227., 347., 249., 372., 840., 797., 904., 864.]), array([254., 382., 258., 396., 920., 893., 990., 967.]), array([ 262.,  394.,  263.,  392., 1004., 1008., 1069., 1083.]), array([ 259.,  384.,  235.,  364., 1087., 1119., 1165., 1203.]), array([ 229.,  359.,  203.,  340., 1187., 1225., 1262., 1290.]), array([ 194.,  331.,  185.,  320., 1289., 1315., 1375., 1385.])]\n",
      "[array([218., 277., 216., 278.,  47.,  48.,  83.,  84.]), array([215., 273., 212., 272.,  99.,  99., 135., 135.]), array([215., 274., 207., 282., 156., 156., 199., 200.]), array([224., 289., 213., 284., 221., 225., 279., 279.]), array([220., 273., 215., 278., 314., 311., 354., 353.]), array([218., 276., 196., 269., 368., 372., 423., 430.]), array([198., 269., 188., 269., 438., 442., 496., 501.]), array([187., 263., 184., 260., 510., 519., 565., 580.]), array([182., 254., 168., 248., 580., 598., 640., 660.]), array([167., 241., 158., 231., 663., 679., 724., 739.]), array([149., 227., 143., 228., 757., 756., 829., 823.]), array([148., 230., 148., 235., 858., 841., 929., 903.]), array([ 152.,  236.,  175.,  255.,  940.,  915., 1027.,  986.]), array([ 188.,  262.,  224.,  300., 1058., 1007., 1117., 1064.]), array([ 244.,  321.,  259.,  348., 1133., 1091., 1194., 1152.]), array([ 268.,  364.,  287.,  385., 1219., 1186., 1288., 1261.]), array([ 292.,  386.,  293.,  388., 1308., 1300., 1347., 1338.])]\n",
      "[array([283., 382., 284., 384.,  66.,  53., 105., 102.]), array([288., 384., 296., 392., 116., 112., 169., 168.]), array([296., 388., 298., 390., 177., 181., 235., 246.]), array([294., 388., 273., 366., 242., 264., 290., 331.]), array([272., 366., 252., 337., 302., 347., 361., 408.]), array([248., 331., 220., 296., 375., 421., 425., 473.]), array([213., 291., 186., 271., 444., 480., 504., 539.]), array([175., 268., 167., 259., 525., 547., 597., 605.]), array([167., 260., 174., 276., 619., 610., 695., 662.]), array([182., 276., 211., 304., 718., 680., 783., 734.]), array([220., 317., 248., 360., 813., 751., 876., 807.]), array([260., 373., 296., 411., 893., 834., 958., 907.]), array([ 303.,  425.,  304.,  442.,  969.,  944., 1043., 1033.]), array([ 305.,  441.,  284.,  419., 1054., 1076., 1116., 1164.]), array([ 274.,  399.,  228.,  353., 1138., 1211., 1205., 1293.]), array([ 198.,  331.,  162.,  298., 1239., 1314., 1314., 1387.]), array([ 117.,  278.,   92.,  278., 1350., 1390., 1442., 1471.])]\n",
      "[array([312., 230., 309., 218., 137., 133., 177., 167.]), array([304., 214., 301., 208., 199., 190., 232., 223.]), array([295., 212., 290., 200., 256., 252., 294., 289.]), array([290., 202., 289., 200., 311., 311., 355., 352.]), array([285., 198., 287., 204., 368., 368., 409., 416.]), array([289., 208., 294., 208., 428., 433., 471., 475.]), array([295., 212., 297., 213., 490., 495., 528., 532.]), array([297., 204., 301., 201., 554., 555., 600., 600.]), array([300., 200., 302., 200., 617., 616., 668., 668.]), array([302., 198., 302., 201., 688., 683., 740., 735.]), array([304., 205., 306., 205., 760., 752., 820., 815.]), array([304., 204., 300., 205., 848., 837., 900., 893.]), array([291., 197., 287., 201., 926., 920., 972., 977.]), array([ 285.,  194.,  289.,  196., 1020., 1012., 1071., 1066.]), array([ 300.,  200.,  296.,  201., 1111., 1114., 1174., 1178.]), array([ 301.,  201.,  324.,  202., 1217., 1214., 1286., 1297.]), array([ 335.,  211.,  339.,  218., 1343., 1348., 1399., 1402.])]\n",
      "[array([204., 295., 215., 300.,  62.,  41., 109.,  87.]), array([222., 305., 234., 324., 121.,  97., 167., 142.]), array([234., 325., 234., 333., 175., 168., 220., 224.]), array([233., 332., 227., 319., 229., 243., 273., 304.]), array([225., 315., 210., 303., 290., 316., 342., 380.]), array([211., 299., 191., 285., 359., 392., 419., 448.]), array([190., 283., 178., 273., 433., 457., 484., 515.]), array([172., 270., 167., 264., 507., 526., 571., 585.]), array([164., 265., 167., 267., 589., 594., 650., 646.]), array([172., 267., 173., 282., 668., 658., 736., 723.]), array([175., 284., 185., 293., 759., 737., 827., 813.]), array([188., 293., 190., 298., 843., 829., 913., 910.]), array([ 191.,  303.,  193.,  315.,  934.,  930., 1013., 1007.]), array([ 196.,  316.,  191.,  323., 1038., 1034., 1109., 1113.]), array([ 192.,  313.,  184.,  314., 1136., 1140., 1211., 1224.]), array([ 179.,  310.,  173.,  306., 1239., 1259., 1319., 1337.]), array([ 169.,  305.,  151.,  295., 1343., 1365., 1428., 1444.])]\n",
      "[array([313., 211., 320., 210.,  75.,  76., 117., 118.]), array([315., 215., 315., 215., 139., 137., 188., 187.]), array([308., 224., 305., 218., 208., 203., 252., 243.]), array([295., 224., 307., 216., 267., 267., 305., 297.]), array([308., 214., 304., 213., 339., 343., 361., 364.]), array([307., 209., 308., 206., 389., 395., 434., 429.]), array([310., 204., 317., 211., 461., 459., 508., 502.]), array([313., 205., 322., 203., 537., 545., 584., 589.]), array([314., 204., 319., 206., 615., 615., 676., 676.]), array([314., 201., 319., 201., 696., 700., 758., 756.]), array([318., 206., 324., 211., 774., 776., 828., 826.]), array([331., 209., 337., 211., 868., 871., 932., 938.]), array([ 340.,  214.,  340.,  212.,  977.,  985., 1050., 1047.]), array([ 337.,  205.,  334.,  211., 1078., 1073., 1127., 1120.]), array([ 333.,  204.,  327.,  199., 1157., 1150., 1219., 1221.]), array([ 323.,  211.,  325.,  201., 1266., 1271., 1343., 1336.]), array([ 333.,  199.,  332.,  194., 1396., 1398., 1451., 1453.])]\n",
      "[array([326., 229., 318., 217.,  34.,  24.,  85.,  75.]), array([313., 220., 317., 217.,  97.,  95., 147., 152.]), array([312., 222., 324., 229., 164., 173., 207., 233.]), array([324., 235., 343., 260., 221., 244., 264., 301.]), array([349., 268., 366., 286., 279., 313., 326., 358.]), array([370., 290., 394., 301., 341., 366., 399., 415.]), array([396., 307., 402., 309., 414., 427., 474., 478.]), array([394., 312., 400., 311., 491., 492., 550., 549.]), array([398., 308., 404., 300., 568., 562., 635., 627.]), array([394., 294., 386., 271., 649., 637., 723., 705.]), array([377., 267., 372., 252., 745., 718., 813., 793.]), array([360., 245., 346., 218., 840., 812., 909., 888.]), array([ 337.,  214.,  328.,  197.,  939.,  917., 1020., 1001.]), array([ 326.,  193.,  319.,  184., 1045., 1030., 1127., 1116.]), array([ 320.,  177.,  328.,  177., 1154., 1153., 1237., 1241.]), array([ 330.,  183.,  337.,  174., 1263., 1269., 1350., 1362.]), array([ 342.,  173.,  365.,  170., 1372., 1377., 1456., 1470.])]\n",
      "[array([296., 392., 297., 382.,  38.,  41.,  88.,  93.]), array([298., 384., 302., 383., 103., 107., 158., 156.]), array([304., 380., 304., 382., 177., 175., 238., 239.]), array([311., 383., 306., 382., 253., 249., 315., 316.]), array([311., 383., 306., 383., 326., 325., 380., 390.]), array([306., 383., 295., 377., 392., 408., 440., 465.]), array([290., 370., 275., 350., 462., 488., 510., 547.]), array([273., 348., 254., 330., 526., 558., 584., 619.]), array([252., 327., 231., 302., 593., 634., 644., 689.]), array([215., 293., 176., 259., 650., 704., 695., 752.]), array([167., 248., 131., 214., 712., 771., 769., 824.]), array([118., 208.,  90., 185., 792., 839., 861., 903.]), array([  79.,  183.,   58.,  164.,  889.,  918.,  970., 1000.]), array([  55.,  162.,   57.,  154., 1008., 1012., 1108., 1093.]), array([  59.,  155.,   83.,  179., 1144., 1105., 1236., 1192.]), array([  96.,  179.,  137.,  219., 1266., 1204., 1354., 1287.]), array([ 153.,  233.,  207.,  286., 1387., 1313., 1457., 1375.])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([239., 140., 234., 146.,  40.,  42.,  73.,  82.]), array([231., 148., 236., 160.,  84., 108., 125., 156.]), array([239., 166., 255., 183., 139., 169., 183., 215.]), array([263., 191., 286., 211., 195., 229., 250., 280.]), array([293., 213., 319., 221., 258., 286., 322., 342.]), array([320., 219., 329., 217., 337., 351., 413., 410.]), array([328., 216., 317., 217., 440., 419., 509., 490.]), array([313., 215., 302., 204., 529., 498., 596., 569.]), array([288., 199., 271., 177., 615., 579., 687., 655.]), array([264., 175., 249., 145., 711., 672., 785., 741.]), array([241., 138., 222., 101., 806., 758., 885., 845.]), array([224., 107., 225.,  90., 869., 846., 946., 938.]), array([ 210.,   70.,  217.,   91.,  961.,  969., 1036., 1080.]), array([ 216.,   89.,  247.,  128., 1051., 1109., 1120., 1188.]), array([ 255.,  146.,  288.,  186., 1141., 1211., 1200., 1273.]), array([ 314.,  197.,  362.,  236., 1218., 1294., 1286., 1369.]), array([ 381.,  246.,  408.,  260., 1336., 1387., 1424., 1471.])]\n",
      "[array([300., 215., 299., 210.,  92.,  88., 123., 123.]), array([301., 217., 300., 220., 147., 147., 180., 181.]), array([297., 220., 293., 214., 208., 209., 238., 246.]), array([295., 214., 301., 226., 266., 267., 314., 321.]), array([302., 226., 320., 232., 336., 338., 368., 380.]), array([320., 234., 318., 252., 389., 395., 433., 445.]), array([315., 243., 330., 254., 459., 465., 507., 516.]), array([339., 253., 342., 260., 524., 536., 574., 585.]), array([345., 262., 355., 275., 592., 604., 653., 656.]), array([358., 276., 373., 275., 667., 672., 720., 730.]), array([373., 277., 379., 274., 746., 748., 831., 829.]), array([366., 269., 368., 280., 855., 846., 894., 887.]), array([369., 274., 354., 265., 932., 920., 989., 969.]), array([ 348.,  249.,  336.,  249., 1029., 1009., 1084., 1068.]), array([ 337.,  218.,  341.,  209., 1128., 1113., 1200., 1188.]), array([ 335.,  210.,  331.,  208., 1233., 1225., 1302., 1296.]), array([ 328.,  214.,  327.,  221., 1336., 1333., 1377., 1364.])]\n",
      "[array([271., 162., 266., 158.,  36.,  29.,  77.,  66.]), array([260., 160., 257., 158.,  89.,  82., 130., 127.]), array([258., 157., 259., 153., 143., 138., 192., 194.]), array([261., 154., 268., 156., 204., 206., 249., 265.]), array([269., 160., 289., 179., 264., 280., 328., 356.]), array([295., 185., 310., 202., 341., 375., 390., 426.]), array([312., 204., 331., 229., 406., 437., 465., 500.]), array([331., 229., 343., 235., 487., 509., 558., 570.]), array([341., 235., 344., 225., 580., 580., 655., 640.]), array([331., 225., 310., 200., 673., 649., 745., 708.]), array([297., 195., 271., 162., 764., 721., 835., 791.]), array([258., 149., 244., 113., 848., 808., 920., 890.]), array([ 244.,  107.,  235.,   91.,  944.,  909., 1008.,  992.]), array([ 235.,   82.,  247.,   92., 1019., 1022., 1084., 1108.]), array([ 247.,   98.,  269.,  121., 1096., 1143., 1155., 1226.]), array([ 279.,  130.,  315.,  168., 1183., 1251., 1263., 1336.]), array([ 350.,  175.,  412.,  197., 1291., 1366., 1362., 1466.])]\n",
      "[array([263., 328., 264., 322.,  52.,  62.,  96., 111.]), array([263., 318., 255., 313., 111., 122., 155., 174.]), array([255., 313., 245., 295., 166., 187., 214., 233.]), array([243., 290., 230., 279., 229., 245., 277., 299.]), array([229., 274., 218., 265., 292., 308., 347., 363.]), array([212., 262., 206., 262., 363., 374., 418., 425.]), array([203., 259., 200., 260., 431., 435., 493., 488.]), array([203., 260., 203., 270., 509., 497., 570., 556.]), array([206., 268., 212., 278., 585., 568., 647., 629.]), array([216., 278., 224., 295., 665., 643., 732., 699.]), array([232., 296., 244., 320., 751., 718., 824., 778.]), array([252., 328., 271., 351., 840., 798., 905., 864.]), array([277., 362., 288., 374., 922., 897., 998., 978.]), array([ 290.,  377.,  288.,  378., 1014., 1014., 1085., 1097.]), array([ 289.,  378.,  272.,  361., 1101., 1137., 1164., 1210.]), array([ 266.,  356.,  243.,  336., 1190., 1238., 1264., 1320.]), array([ 238.,  331.,  203.,  307., 1297., 1345., 1372., 1410.])]\n",
      "[array([233., 297., 238., 300.,  56.,  53.,  90.,  93.]), array([235., 294., 237., 290., 109., 109., 152., 156.]), array([237., 290., 229., 281., 172., 177., 226., 238.]), array([227., 276., 216., 267., 243., 252., 300., 313.]), array([215., 265., 210., 254., 326., 336., 381., 393.]), array([208., 254., 192., 250., 399., 408., 457., 464.]), array([197., 246., 188., 252., 480., 477., 540., 540.]), array([197., 248., 191., 261., 566., 558., 622., 614.]), array([199., 260., 203., 275., 648., 638., 703., 692.]), array([214., 267., 222., 287., 732., 720., 791., 765.]), array([225., 290., 239., 298., 814., 792., 865., 854.]), array([235., 295., 238., 315., 897., 882., 963., 957.]), array([ 252.,  330.,  246.,  325.,  995., 1000., 1058., 1067.]), array([ 245.,  324.,  241.,  308., 1081., 1102., 1142., 1163.]), array([ 228.,  307.,  226.,  289., 1186., 1203., 1254., 1279.]), array([ 203.,  288.,  189.,  271., 1295., 1321., 1378., 1395.]), array([ 183.,  271.,  179.,  275., 1427., 1430., 1469., 1475.])]\n",
      "[array([308., 193., 311., 208.,  29.,  54.,  75., 101.]), array([316., 208., 327., 217.,  89., 112., 149., 178.]), array([327., 218., 339., 243., 159., 184., 221., 245.]), array([341., 240., 348., 264., 230., 257., 283., 313.]), array([356., 266., 369., 279., 298., 321., 353., 379.]), array([374., 281., 387., 294., 368., 391., 425., 452.]), array([390., 296., 404., 306., 442., 460., 505., 522.]), array([409., 311., 414., 314., 524., 532., 589., 589.]), array([416., 317., 413., 314., 611., 601., 683., 657.]), array([411., 311., 396., 290., 702., 670., 768., 729.]), array([391., 278., 367., 256., 798., 747., 853., 803.]), array([361., 247., 334., 223., 880., 829., 945., 897.]), array([ 326.,  219.,  308.,  186.,  967.,  923., 1042.,  996.]), array([ 297.,  183.,  285.,  158., 1067., 1032., 1140., 1113.]), array([ 276.,  149.,  275.,  144., 1166., 1150., 1252., 1243.]), array([ 274.,  142.,  282.,  134., 1282., 1278., 1361., 1365.]), array([ 285.,  139.,  292.,  139., 1380., 1382., 1461., 1465.])]\n",
      "[array([299., 204., 285., 204.,  43.,  39.,  89.,  82.]), array([282., 202., 280., 199., 108., 104., 151., 146.]), array([282., 201., 286., 199., 170., 173., 223., 232.]), array([293., 201., 304., 201., 244., 253., 299., 320.]), array([306., 209., 310., 219., 322., 328., 380., 396.]), array([310., 219., 325., 216., 401., 413., 458., 469.]), array([326., 217., 332., 222., 476., 484., 540., 540.]), array([334., 225., 334., 222., 559., 556., 623., 614.]), array([330., 221., 328., 217., 641., 639., 718., 701.]), array([328., 214., 324., 203., 738., 718., 804., 782.]), array([318., 197., 308., 194., 828., 823., 893., 885.]), array([311., 197., 305., 194., 920., 915., 983., 977.]), array([ 300.,  196.,  303.,  194., 1005., 1004., 1075., 1073.]), array([ 298.,  191.,  299.,  192., 1114., 1110., 1179., 1177.]), array([ 302.,  180.,  309.,  172., 1220., 1211., 1294., 1292.]), array([ 309.,  168.,  306.,  171., 1333., 1321., 1392., 1372.]), array([ 308.,  165.,  308.,  160., 1416., 1413., 1489., 1482.])]\n",
      "[array([344., 240., 341., 246.,  37.,  38.,  71.,  76.]), array([337., 240., 340., 247.,  95.,  97., 138., 139.]), array([333., 246., 333., 248., 168., 161., 215., 210.]), array([327., 247., 326., 247., 241., 237., 281., 276.]), array([318., 247., 321., 245., 314., 305., 358., 349.]), array([317., 246., 317., 240., 383., 377., 425., 419.]), array([316., 240., 313., 229., 457., 447., 500., 488.]), array([305., 230., 300., 225., 521., 519., 566., 563.]), array([301., 221., 298., 220., 595., 592., 644., 641.]), array([297., 216., 303., 217., 668., 664., 718., 719.]), array([302., 217., 298., 216., 743., 741., 796., 800.]), array([300., 217., 304., 217., 828., 825., 883., 882.]), array([304., 220., 308., 224., 919., 918., 978., 985.]), array([ 302.,  221.,  301.,  220., 1026., 1019., 1098., 1091.]), array([ 304.,  216.,  296.,  207., 1144., 1131., 1212., 1198.]), array([ 297.,  202.,  290.,  197., 1253., 1245., 1327., 1319.]), array([ 290.,  186.,  290.,  181., 1377., 1365., 1420., 1407.])]\n",
      "0 torch.Size([2, 3, 1536, 512]) torch.Size([2, 17, 5]) torch.Size([2, 17, 8])\n",
      "[array([229., 302., 231., 298.,  76.,  66., 108.,  95.]), array([236., 302., 238., 307., 124., 113., 162., 157.]), array([246., 310., 249., 308., 179., 185., 227., 240.]), array([243., 303., 231., 293., 245., 264., 297., 323.]), array([230., 291., 211., 278., 310., 340., 355., 391.]), array([212., 270., 194., 250., 379., 407., 431., 457.]), array([195., 245., 177., 234., 450., 468., 502., 521.]), array([178., 230., 164., 220., 524., 531., 586., 588.]), array([174., 224., 167., 231., 607., 602., 669., 661.]), array([170., 232., 179., 248., 691., 675., 756., 733.]), array([185., 251., 193., 263., 779., 753., 850., 824.]), array([208., 275., 217., 292., 873., 852., 941., 911.]), array([ 220.,  301.,  236.,  318.,  967.,  949., 1033., 1020.]), array([ 238.,  321.,  239.,  324., 1066., 1063., 1142., 1137.]), array([ 243.,  324.,  232.,  312., 1166., 1179., 1237., 1257.]), array([ 225.,  310.,  207.,  300., 1264., 1289., 1341., 1371.]), array([ 192.,  293.,  190.,  293., 1407., 1418., 1463., 1469.])]\n"
     ]
    }
   ],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    print(idx ,images.shape, annotations.shape, corners.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "model = EfficientDet(num_classes=num_classes,\n",
    "                     network=network,\n",
    "                     W_bifpn=EFFICIENTDET[network]['W_bifpn'],\n",
    "                     D_bifpn=EFFICIENTDET[network]['D_bifpn'],\n",
    "                     D_class=EFFICIENTDET[network]['D_class'],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(resume is not None):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "device, device_ids = prepare_device(device)\n",
    "model = model.to(device)\n",
    "if(len(device_ids) > 1):\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "if(resume is not None):\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=3, verbose=True)\n",
    "criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t start training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnav0400/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce17da478d84860af54e90ab854cf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/arnav0400/Spine-Project/EfficientDet.Pytorch/datasets/Spine_dataset.py\", line 40, in __getitem__\n    augmentation = self.transform(**annotation)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/core/composition.py\", line 174, in __call__\n    p.preprocess(data)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 63, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 71, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 77, in convert_to_albumentations\n    angle_in_degrees=self.params.angle_in_degrees,\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 187, in convert_keypoints_to_albumentations\n    for kp in keypoints\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 187, in <listcomp>\n    for kp in keypoints\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 148, in convert_keypoint_to_albumentations\n    check_keypoint(keypoint, rows, cols)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 87, in check_keypoint\n    \"to be in the range [0.0, {size}], got {value}.\".format(kp=kp, name=name, value=value, size=size)\nValueError: Expected y for keypoint (604.0, 2325.0, 0.0, 0) to be in the range [0.0, 2325], got 2325.0.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f7dd094d8e5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtk0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_corners\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mannotations_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations_bboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/arnav0400/Spine-Project/EfficientDet.Pytorch/datasets/Spine_dataset.py\", line 40, in __getitem__\n    augmentation = self.transform(**annotation)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/core/composition.py\", line 174, in __call__\n    p.preprocess(data)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 63, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 71, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 77, in convert_to_albumentations\n    angle_in_degrees=self.params.angle_in_degrees,\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 187, in convert_keypoints_to_albumentations\n    for kp in keypoints\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 187, in <listcomp>\n    for kp in keypoints\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 148, in convert_keypoint_to_albumentations\n    check_keypoint(keypoint, rows, cols)\n  File \"/home/arnav0400/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/keypoints_utils.py\", line 87, in check_keypoint\n    \"to be in the range [0.0, {size}], got {value}.\".format(kp=kp, name=name, value=value, size=size)\nValueError: Expected y for keypoint (604.0, 2325.0, 0.0, 0) to be in the range [0.0, 2325], got 2325.0.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "best_loss = 100\n",
    "df = pd.DataFrame(np.zeros((num_epochs,6)),columns = [\"train_cls\",\"train_bbox_loss\",\"train_corner_loss\",\"val_cls\",\"val_bbox_loss\",\"val_corner_loss\"])\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"{} epoch: \\t start training....\".format(epoch))\n",
    "    start = time.time()\n",
    "    result = {}\n",
    "    total_loss = []\n",
    "    bbox_losses = []\n",
    "    cls_losses = []\n",
    "    corner_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    total_batches = len(train_dataloader)\n",
    "    tk0 = tqdm(train_dataloader, total=total_batches)\n",
    "    for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "        images = images.to(device)\n",
    "        annotations_bboxes = annotations_bboxes.to(device)\n",
    "        annotations_corners = annotations_corners.to(device)\n",
    "        classification, regression, corners, anchors = model(images)\n",
    "        classification_loss, regression_loss, corner_loss= criterion(\n",
    "            classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "        corner_loss = corner_loss.mean()\n",
    "        loss = classification_loss + regression_loss + corner_loss\n",
    "        if bool(loss == 0):\n",
    "            print('loss equal zero(0)')\n",
    "            continue\n",
    "        loss.backward()\n",
    "        if (idx+1) % grad_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_loss.append(loss.item())\n",
    "        corner_losses.append(corner_loss.item())\n",
    "        bbox_losses.append(regression_loss.item())\n",
    "        cls_losses.append(classification_loss.item())\n",
    "        tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "    result = {\n",
    "        'time': time.time() - start,\n",
    "        'loss': np.mean(total_loss),\n",
    "        'corner_loss': np.mean(corner_losses),\n",
    "        'bbox_loss': np.mean(bbox_losses),\n",
    "        'cls_loss': np.mean(cls_losses)\n",
    "    }\n",
    "    for key, value in result.items():\n",
    "        print('    {:15s}: {}'.format(str(key), value))\n",
    "    df.iloc[epoch,:3] = [np.mean(cls_losses),np.mean(bbox_losses),np.mean(corner_losses)] \n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        result = {}\n",
    "        total_loss = []\n",
    "        bbox_losses = []\n",
    "        cls_losses = []\n",
    "        corner_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        total_batches = len(test_dataloader)\n",
    "        tk0 = tqdm(val_dataloader, total=total_batches)\n",
    "        for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "            images = images.to(device)\n",
    "            annotations_bboxes = annotations_bboxes.to(device)\n",
    "            annotations_corners = annotations_corners.to(device)\n",
    "            classification, regression, corners, anchors = model(images)\n",
    "            classification_loss, regression_loss, corner_loss= criterion(\n",
    "                classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "            corner_loss = corner_loss.mean()\n",
    "            loss = classification_loss + regression_loss + corner_loss\n",
    "            if bool(loss == 0):\n",
    "                print('loss equal zero(0)')\n",
    "                continue\n",
    "            total_loss.append(loss.item())\n",
    "            corner_losses.append(corner_loss.item())\n",
    "            bbox_losses.append(regression_loss.item())\n",
    "            cls_losses.append(classification_loss.item())\n",
    "            tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "        result = {\n",
    "            'time': time.time() - start,\n",
    "            'loss': np.mean(total_loss),\n",
    "            'corner_loss': np.mean(corner_losses),\n",
    "            'bbox_loss': np.mean(bbox_losses),\n",
    "            'cls_loss': np.mean(cls_losses)\n",
    "        }\n",
    "        for key, value in result.items():\n",
    "            print('    {:15s}: {}'.format(str(key), value))\n",
    "            \n",
    "    scheduler.step(np.mean(total_loss))\n",
    "    df.iloc[epoch,3:] = [np.mean(cls_losses),np.mean(bbox_losses),np.mean(corner_losses)]\n",
    "    df.to_csv('../logs/d5-resize-corner-(1536,512).csv')\n",
    "    torch.cuda.empty_cache()\n",
    "    arch = type(model).__name__\n",
    "    state = {\n",
    "        'arch': arch,\n",
    "        'num_class': num_classes,\n",
    "        'network': network,\n",
    "        'state_dict': get_state_dict(model),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    if(best_loss>np.mean(corner_losses)):\n",
    "        best_loss = np.mean(corner_losses)\n",
    "        torch.save(state, '../weights/d5-resize-corner-(1536,512).pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(dataloader,split = 'test'):\n",
    "    if split=='test':\n",
    "        for idx, (images) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            image = images[0]\n",
    "            classification_score, category, boxes, corners = model(images)\n",
    "            break\n",
    "    else:\n",
    "        for idx, (images, annotations, corners) in enumerate(dataloader):\n",
    "            image = images[0]\n",
    "            images = images.to(device)\n",
    "#             classification_score, category, boxes, corners = model(images)\n",
    "            break\n",
    "    corners = corners.detach().cpu().numpy()\n",
    "    boxes = boxes.detach().cpu().numpy()\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    boxes = boxes.astype(np.int16)\n",
    "    for box in boxes:\n",
    "        start = (box[0],box[1])\n",
    "        end = (box[2],box[3])\n",
    "        image = cv2.rectangle(image, start, end, (255,0,0), 10)\n",
    "    plt.scatter(corners[:,:4],corners[:,4:])\n",
    "    plt.imshow(image.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    image = images[0]\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    corners = corners[0].detach().cpu().numpy()\n",
    "    corners = corners.astype(np.int16)\n",
    "    plt.scatter(corners[:,:4],corners[:,4:])\n",
    "    plt.imshow(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(train_dataloader,'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(dataloader = val_dataloader):\n",
    "    corners = []\n",
    "    clipped_corners = []\n",
    "    \n",
    "    # Predictions from model\n",
    "    for idx, (image, annotation, corner) in enumerate(val_dataloader):\n",
    "        image = image.to(device)\n",
    "        classification_score, category, pred_boxes, pred_corner = model(image)\n",
    "        classification_score = classification_score.detach().cpu().numpy()\n",
    "        pred_corner = pred_corner.detach().cpu().numpy()\n",
    "        if classification_score.shape[0]>17:\n",
    "            ind = np.argpartition(classification_score, -17)[-17:]\n",
    "            corners.append(pred_corner[ind])\n",
    "        else:\n",
    "            corners.append(pred_corner)\n",
    "    \n",
    "    #Making all predictions 17\n",
    "    for corner in corners:\n",
    "        if corner.shape[0]==17:\n",
    "            clipped_corners.append(corner)\n",
    "        else:\n",
    "            num_repeat = 17-corner.shape[0]\n",
    "            repeat = corner[-1].reshape(1,8)\n",
    "            for i in range(num_repeat):\n",
    "                corner = np.append(corner,repeat,axis=0)\n",
    "            clipped_corners.append(corner)\n",
    "    \n",
    "    clipped_corners = np.array(clipped_corners)\n",
    "    \n",
    "    #Reshaping and saving csv\n",
    "    val_landmarks = np.zeros((len(clipped_corners),136))\n",
    "    for i in range(len(clipped_corners)):\n",
    "        val_landmarks[i,:68] = clipped_corners[i,:,:4].reshape(1,68)/768\n",
    "        val_landmarks[i,68:] = clipped_corners[i,:,4:].reshape(1,68)/1408\n",
    "    \n",
    "    val_preds = pd.DataFrame(val_landmarks)\n",
    "    val_preds.to_csv('val_preds.csv',header = None,index = False)\n",
    "    return val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## find the SMAPE error\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.sum(np.abs(y_true-y_pred), axis=1)\n",
    "    denominator = np.sum(np.abs(y_true+y_pred), axis=1)\n",
    "    smape_val = np.mean(numerator/ denominator) * 100\n",
    "    return smape_val\n",
    "\n",
    "\n",
    "\n",
    "true_csv_path = \"boostnet_labeldata/labels/test/angles.csv\"\n",
    "pred_csv_path = \"boostnet_labeldata/labels/test/test_angles_polyfit.csv\"\n",
    "true_angles = pd.read_csv(true_csv_path, header=None).values\n",
    "pred_angles = pd.read_csv(pred_csv_path, header=None).values\n",
    "print (smape(true_angles, pred_angles))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
