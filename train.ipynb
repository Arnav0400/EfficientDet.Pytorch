{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import Spine_dataset, get_augumentation, detection_collate\n",
    "from utils import EFFICIENTDET\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = None#'weights/checkpoint_efficientdet-d3_33.pth'\n",
    "network = 'efficientdet-d5'\n",
    "num_epochs = 100\n",
    "batch_size = 2\n",
    "num_worker = 4\n",
    "num_classes = 1\n",
    "device = [0]\n",
    "grad_accumulation_steps = 1\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "save_folder = '../storage/weights/'\n",
    "image_root = 'boostnet_labeldata/data/'\n",
    "csv_root = 'boostnet_labeldata/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_device(device):\n",
    "    n_gpu_use = len(device)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu_use > 0 and n_gpu == 0:\n",
    "        print(\"Warning: There\\'s no GPU available on this machine, training will be performed on CPU.\")\n",
    "        n_gpu_use = 0\n",
    "    if n_gpu_use > n_gpu:\n",
    "        print(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available on this machine.\".format(\n",
    "            n_gpu_use, n_gpu))\n",
    "        n_gpu_use = n_gpu\n",
    "    list_ids = device\n",
    "    device = torch.device('cuda:{}'.format(\n",
    "        device[0]) if n_gpu_use > 0 else 'cpu')\n",
    "\n",
    "    return device, list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict(model):\n",
    "    if type(model) == torch.nn.DataParallel:\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "if(resume is not None):\n",
    "    resume_path = str(resume)\n",
    "    print(\"Loading checkpoint: {} ...\".format(resume_path))\n",
    "    checkpoint = torch.load(\n",
    "        resume, map_location=lambda storage, loc: storage)\n",
    "    num_classes = checkpoint['num_class']\n",
    "    network = checkpoint['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_train = pd.read_csv(csv_root+'training/landmarks.csv',header = None)\n",
    "filename_df_train = pd.read_csv(csv_root+'training/filenames.csv',header = None)\n",
    "boxes_df_train = pd.read_csv(csv_root+'training/train.csv')\n",
    "boxes_df_train.label = 0 # All boxes same class??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_test = pd.read_csv(csv_root+'test/landmarks.csv',header = None)\n",
    "filename_df_test = pd.read_csv(csv_root+'test/filenames.csv',header = None)\n",
    "boxes_df_test = pd.read_csv('test.csv')\n",
    "boxes_df_test.label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.47815</td>\n",
       "      <td>0.66623</td>\n",
       "      <td>0.46490</td>\n",
       "      <td>0.65960</td>\n",
       "      <td>0.46358</td>\n",
       "      <td>0.63841</td>\n",
       "      <td>0.45430</td>\n",
       "      <td>0.61854</td>\n",
       "      <td>0.45828</td>\n",
       "      <td>0.62517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.79859</td>\n",
       "      <td>0.81318</td>\n",
       "      <td>0.81929</td>\n",
       "      <td>0.84376</td>\n",
       "      <td>0.86541</td>\n",
       "      <td>0.89365</td>\n",
       "      <td>0.88894</td>\n",
       "      <td>0.91482</td>\n",
       "      <td>0.93694</td>\n",
       "      <td>0.95294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50557</td>\n",
       "      <td>0.65584</td>\n",
       "      <td>0.48887</td>\n",
       "      <td>0.64935</td>\n",
       "      <td>0.48145</td>\n",
       "      <td>0.64471</td>\n",
       "      <td>0.46753</td>\n",
       "      <td>0.64007</td>\n",
       "      <td>0.45269</td>\n",
       "      <td>0.62059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78511</td>\n",
       "      <td>0.80880</td>\n",
       "      <td>0.80203</td>\n",
       "      <td>0.84010</td>\n",
       "      <td>0.85829</td>\n",
       "      <td>0.89382</td>\n",
       "      <td>0.88663</td>\n",
       "      <td>0.91709</td>\n",
       "      <td>0.94205</td>\n",
       "      <td>0.95516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.30297</td>\n",
       "      <td>0.60792</td>\n",
       "      <td>0.30297</td>\n",
       "      <td>0.59406</td>\n",
       "      <td>0.31485</td>\n",
       "      <td>0.57624</td>\n",
       "      <td>0.31683</td>\n",
       "      <td>0.56634</td>\n",
       "      <td>0.32673</td>\n",
       "      <td>0.56634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78249</td>\n",
       "      <td>0.79125</td>\n",
       "      <td>0.85791</td>\n",
       "      <td>0.82020</td>\n",
       "      <td>0.88956</td>\n",
       "      <td>0.83906</td>\n",
       "      <td>0.93939</td>\n",
       "      <td>0.86869</td>\n",
       "      <td>0.95354</td>\n",
       "      <td>0.89024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50095</td>\n",
       "      <td>0.75810</td>\n",
       "      <td>0.50476</td>\n",
       "      <td>0.74857</td>\n",
       "      <td>0.52762</td>\n",
       "      <td>0.74286</td>\n",
       "      <td>0.51048</td>\n",
       "      <td>0.73905</td>\n",
       "      <td>0.51810</td>\n",
       "      <td>0.75619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.89504</td>\n",
       "      <td>0.87023</td>\n",
       "      <td>0.93511</td>\n",
       "      <td>0.90331</td>\n",
       "      <td>0.89059</td>\n",
       "      <td>0.86959</td>\n",
       "      <td>0.92303</td>\n",
       "      <td>0.89377</td>\n",
       "      <td>0.94529</td>\n",
       "      <td>0.92303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.40951</td>\n",
       "      <td>0.67824</td>\n",
       "      <td>0.41499</td>\n",
       "      <td>0.67642</td>\n",
       "      <td>0.40768</td>\n",
       "      <td>0.65996</td>\n",
       "      <td>0.42596</td>\n",
       "      <td>0.65265</td>\n",
       "      <td>0.43876</td>\n",
       "      <td>0.63803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84701</td>\n",
       "      <td>0.82523</td>\n",
       "      <td>0.86930</td>\n",
       "      <td>0.85208</td>\n",
       "      <td>0.91084</td>\n",
       "      <td>0.89970</td>\n",
       "      <td>0.93414</td>\n",
       "      <td>0.92300</td>\n",
       "      <td>0.97670</td>\n",
       "      <td>0.96707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2        3        4        5        6        7    \\\n",
       "0  0.47815  0.66623  0.46490  0.65960  0.46358  0.63841  0.45430  0.61854   \n",
       "1  0.50557  0.65584  0.48887  0.64935  0.48145  0.64471  0.46753  0.64007   \n",
       "2  0.30297  0.60792  0.30297  0.59406  0.31485  0.57624  0.31683  0.56634   \n",
       "3  0.50095  0.75810  0.50476  0.74857  0.52762  0.74286  0.51048  0.73905   \n",
       "4  0.40951  0.67824  0.41499  0.67642  0.40768  0.65996  0.42596  0.65265   \n",
       "\n",
       "       8        9    ...      126      127      128      129      130  \\\n",
       "0  0.45828  0.62517  ...  0.79859  0.81318  0.81929  0.84376  0.86541   \n",
       "1  0.45269  0.62059  ...  0.78511  0.80880  0.80203  0.84010  0.85829   \n",
       "2  0.32673  0.56634  ...  0.78249  0.79125  0.85791  0.82020  0.88956   \n",
       "3  0.51810  0.75619  ...  0.89504  0.87023  0.93511  0.90331  0.89059   \n",
       "4  0.43876  0.63803  ...  0.84701  0.82523  0.86930  0.85208  0.91084   \n",
       "\n",
       "       131      132      133      134      135  \n",
       "0  0.89365  0.88894  0.91482  0.93694  0.95294  \n",
       "1  0.89382  0.88663  0.91709  0.94205  0.95516  \n",
       "2  0.83906  0.93939  0.86869  0.95354  0.89024  \n",
       "3  0.86959  0.92303  0.89377  0.94529  0.92303  \n",
       "4  0.89970  0.93414  0.92300  0.97670  0.96707  \n",
       "\n",
       "[5 rows x 136 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_train,corner_df_train,filename_df_train,transform=get_augumentation('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_test,corner_df_test,filename_df_test,transform=get_augumentation('val'),image_set='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Spine_dataset.SPINEDetection_test(transform=get_augumentation('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=None,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/notebooks/EfficientDet.Pytorch/datasets/augmentation.py\", line 52, in detection_collate\n    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded), torch.FloatTensor(corners))\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2377 and 2575 in dimension 2 at /pytorch/aten/src/TH/generic/THTensor.cpp:691\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-58f7c6b9d7fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/notebooks/EfficientDet.Pytorch/datasets/augmentation.py\", line 52, in detection_collate\n    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded), torch.FloatTensor(corners))\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2377 and 2575 in dimension 2 at /pytorch/aten/src/TH/generic/THTensor.cpp:691\n"
     ]
    }
   ],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    print(idx ,images.shape, annotations.shape, corners.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    print(idx ,images.shape, annotations.shape, corners.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientDet(num_classes=num_classes,\n",
    "                     network=network,\n",
    "                     W_bifpn=EFFICIENTDET[network]['W_bifpn'],\n",
    "                     D_bifpn=EFFICIENTDET[network]['D_bifpn'],\n",
    "                     D_class=EFFICIENTDET[network]['D_class'],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(resume is not None):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "device, device_ids = prepare_device(device)\n",
    "model = model.to(device)\n",
    "if(len(device_ids) > 1):\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=3, verbose=True)\n",
    "criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "best_loss = 100\n",
    "df = pd.DataFrame(np.zeros((num_epochs,6)),columns = [\"train_cls\",\"train_bbox_loss\",\"train_corner_loss\",\"val_cls\",\"val_bbox_loss\",\"val_corner_loss\"])\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"{} epoch: \\t start training....\".format(epoch))\n",
    "    \n",
    "    start = time.time()\n",
    "    result = {}\n",
    "    total_loss = []\n",
    "    bbox_losses = []\n",
    "    cls_losses = []\n",
    "    corner_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    total_batches = len(train_dataloader)\n",
    "    tk0 = tqdm(train_dataloader, total=total_batches)\n",
    "    for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "        images = images.to(device)\n",
    "        annotations_bboxes = annotations_bboxes.to(device)\n",
    "        annotations_corners = annotations_corners.to(device)\n",
    "        classification, regression, corners, anchors = model(images)\n",
    "        classification_loss, regression_loss, corner_loss= criterion(\n",
    "            classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "        corner_loss = corner_loss.mean()\n",
    "        loss = classification_loss + regression_loss + corner_loss\n",
    "        if bool(loss == 0):\n",
    "            print('loss equal zero(0)')\n",
    "            continue\n",
    "        loss.backward()\n",
    "        if (idx+1) % grad_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_loss.append(loss.item())\n",
    "        corner_losses.append(corner_loss.item())\n",
    "        bbox_losses.append(regression_loss.item())\n",
    "        cls_losses.append(classification_loss.item())\n",
    "        tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "    result = {\n",
    "        'time': time.time() - start,\n",
    "        'loss': np.mean(total_loss),\n",
    "        'corner_loss': np.mean(corner_losses),\n",
    "        'bbox_loss': np.mean(bbox_losses),\n",
    "        'cls_loss': np.mean(cls_losses)\n",
    "    }\n",
    "    for key, value in result.items():\n",
    "        print('    {:15s}: {}'.format(str(key), value))\n",
    "    df.iloc[epoch,:3] = [np.mean(cls_losses),np.mean(bbox_losses),np.mean(corner_losses)] \n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        result = {}\n",
    "        total_loss = []\n",
    "        bbox_losses = []\n",
    "        cls_losses = []\n",
    "        corner_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        total_batches = len(test_dataloader)\n",
    "        tk0 = tqdm(val_dataloader, total=total_batches)\n",
    "        for idx, (images, annotations_bboxes, annotations_corners) in enumerate(tk0):\n",
    "            images = images.to(device)\n",
    "            annotations_bboxes = annotations_bboxes.to(device)\n",
    "            annotations_corners = annotations_corners.to(device)\n",
    "            classification, regression, corners, anchors = model(images)\n",
    "            classification_loss, regression_loss, corner_loss= criterion(\n",
    "                classification, regression, corners, anchors, annotations_bboxes, annotations_corners)\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "            corner_loss = corner_loss.mean()\n",
    "            loss = classification_loss + regression_loss + corner_loss\n",
    "            if bool(loss == 0):\n",
    "                print('loss equal zero(0)')\n",
    "                continue\n",
    "            total_loss.append(loss.item())\n",
    "            corner_losses.append(corner_loss.item())\n",
    "            bbox_losses.append(regression_loss.item())\n",
    "            cls_losses.append(classification_loss.item())\n",
    "            tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "        result = {\n",
    "            'time': time.time() - start,\n",
    "            'loss': np.mean(total_loss),\n",
    "            'corner_loss': np.mean(corner_losses),\n",
    "            'bbox_loss': np.mean(bbox_losses),\n",
    "            'cls_loss': np.mean(cls_losses)\n",
    "        }\n",
    "        for key, value in result.items():\n",
    "            print('    {:15s}: {}'.format(str(key), value))\n",
    "            \n",
    "    scheduler.step(np.mean(total_loss))\n",
    "    df.iloc[epoch,3:] = [np.mean(cls_losses),np.mean(bbox_losses),np.mean(corner_losses)]\n",
    "    df.to_csv('../storage/cd5-resize-(1536,512)px.csv')\n",
    "    torch.cuda.empty_cache()\n",
    "    arch = type(model).__name__\n",
    "    state = {\n",
    "        'arch': arch,\n",
    "        'num_class': num_classes,\n",
    "        'network': network,\n",
    "        'state_dict': get_state_dict(model)\n",
    "    }\n",
    "    if(best_loss>np.mean(corner_losses)):\n",
    "        best_loss = np.mean(corner_losses)\n",
    "        torch.save(\n",
    "            state, './storage/cd5-resize-(1536,512)px.pth'.format(network, epoch))\n",
    "#     state = {\n",
    "#     'arch': arch,\n",
    "#     'num_class': num_classes,\n",
    "#     'network': network,\n",
    "#     'state_dict': get_state_dict(model)\n",
    "# }\n",
    "# torch.save(state, './weights/Final_{}.pth'.format(network))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(dataloader,split = 'test'):\n",
    "    if split=='test':\n",
    "        for idx, (images) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            image = images[0]\n",
    "            classification_score, category, boxes, corners = model(images)\n",
    "            break\n",
    "    else:\n",
    "        for idx, (images, annotations, corners) in enumerate(dataloader):\n",
    "            image = images[0]\n",
    "            images = images.to(device)\n",
    "#             classification_score, category, boxes, corners = model(images)\n",
    "            break\n",
    "    corners = corners.detach().cpu().numpy()\n",
    "    boxes = boxes.detach().cpu().numpy()\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    boxes = boxes.astype(np.int16)\n",
    "    for box in boxes:\n",
    "        start = (box[0],box[1])\n",
    "        end = (box[2],box[3])\n",
    "        image = cv2.rectangle(image, start, end, (255,0,0), 10)\n",
    "    plt.scatter(corners[:,:4],corners[:,4:])\n",
    "    plt.imshow(image.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (images, annotations, corners) in enumerate(train_dataloader):\n",
    "    image = images[0]\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    corners = corners[0].detach().cpu().numpy()\n",
    "    corners = corners.astype(np.int16)\n",
    "    plt.scatter(corners[:,:4],corners[:,4:])\n",
    "    plt.imshow(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(train_dataloader,'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(dataloader = val_dataloader):\n",
    "    corners = []\n",
    "    clipped_corners = []\n",
    "    \n",
    "    # Predictions from model\n",
    "    for idx, (image, annotation, corner) in enumerate(val_dataloader):\n",
    "        image = image.to(device)\n",
    "        classification_score, category, pred_boxes, pred_corner = model(image)\n",
    "        classification_score = classification_score.detach().cpu().numpy()\n",
    "        pred_corner = pred_corner.detach().cpu().numpy()\n",
    "        if classification_score.shape[0]>17:\n",
    "            ind = np.argpartition(classification_score, -17)[-17:]\n",
    "            corners.append(pred_corner[ind])\n",
    "        else:\n",
    "            corners.append(pred_corner)\n",
    "    \n",
    "    #Making all predictions 17\n",
    "    for corner in corners:\n",
    "        if corner.shape[0]==17:\n",
    "            clipped_corners.append(corner)\n",
    "        else:\n",
    "            num_repeat = 17-corner.shape[0]\n",
    "            repeat = corner[-1].reshape(1,8)\n",
    "            for i in range(num_repeat):\n",
    "                corner = np.append(corner,repeat,axis=0)\n",
    "            clipped_corners.append(corner)\n",
    "    \n",
    "    clipped_corners = np.array(clipped_corners)\n",
    "    \n",
    "    #Reshaping and saving csv\n",
    "    val_landmarks = np.zeros((len(clipped_corners),136))\n",
    "    for i in range(len(clipped_corners)):\n",
    "        val_landmarks[i,:68] = clipped_corners[i,:,:4].reshape(1,68)/768\n",
    "        val_landmarks[i,68:] = clipped_corners[i,:,4:].reshape(1,68)/1408\n",
    "    \n",
    "    val_preds = pd.DataFrame(val_landmarks)\n",
    "    val_preds.to_csv('val_preds.csv',header = None,index = False)\n",
    "    return val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## find the SMAPE error\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.sum(np.abs(y_true-y_pred), axis=1)\n",
    "    denominator = np.sum(np.abs(y_true+y_pred), axis=1)\n",
    "    smape_val = np.mean(numerator/ denominator) * 100\n",
    "    return smape_val\n",
    "\n",
    "\n",
    "\n",
    "true_csv_path = \"boostnet_labeldata/labels/test/angles.csv\"\n",
    "pred_csv_path = \"boostnet_labeldata/labels/test/test_angles_polyfit.csv\"\n",
    "true_angles = pd.read_csv(true_csv_path, header=None).values\n",
    "pred_angles = pd.read_csv(pred_csv_path, header=None).values\n",
    "print (smape(true_angles, pred_angles))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
