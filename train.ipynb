{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import Spine_dataset, get_augumentation, detection_collate\n",
    "from utils import EFFICIENTDET\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "#     random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = None#'../weights/d4-downscale-(1536,512).pth'\n",
    "network = 'efficientdet-d4'\n",
    "num_epochs = 40\n",
    "batch_size = 1\n",
    "num_worker = 4\n",
    "num_classes = 1\n",
    "device = [0]\n",
    "grad_accumulation_steps = 1\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "save_folder = 'weights/'\n",
    "image_root = 'boostnet_labeldata/data/'\n",
    "csv_root = 'boostnet_labeldata/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_device(device):\n",
    "    n_gpu_use = len(device)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu_use > 0 and n_gpu == 0:\n",
    "        print(\"Warning: There\\'s no GPU available on this machine, training will be performed on CPU.\")\n",
    "        n_gpu_use = 0\n",
    "    if n_gpu_use > n_gpu:\n",
    "        print(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available on this machine.\".format(\n",
    "            n_gpu_use, n_gpu))\n",
    "        n_gpu_use = n_gpu\n",
    "    list_ids = device\n",
    "    device = torch.device('cuda:{}'.format(\n",
    "        device[0]) if n_gpu_use > 0 else 'cpu')\n",
    "\n",
    "    return device, list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict(model):\n",
    "    if type(model) == torch.nn.DataParallel:\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "if(resume is not None):\n",
    "    resume_path = str(resume)\n",
    "    print(\"Loading checkpoint: {} ...\".format(resume_path))\n",
    "    checkpoint = torch.load(\n",
    "        resume, map_location=lambda storage, loc: storage)\n",
    "    num_classes = checkpoint['num_class']\n",
    "    network = checkpoint['network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_train = pd.read_csv(csv_root+'training/landmarks.csv',header = None)\n",
    "filename_df_train = pd.read_csv(csv_root+'training/filenames.csv',header = None)\n",
    "boxes_df_train = pd.read_csv(csv_root+'training/train.csv')\n",
    "boxes_df_train.label = 0 # All boxes same class??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_df_test = pd.read_csv(csv_root+'test/landmarks.csv',header = None)\n",
    "filename_df_test = pd.read_csv(csv_root+'test/filenames.csv',header = None)\n",
    "boxes_df_test = pd.read_csv('test.csv')\n",
    "boxes_df_test.label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_train,filename_df_train,transform=get_augumentation('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Spine_dataset.SPINEDetection(image_root,boxes_df_test,filename_df_test,transform=get_augumentation('val'),image_set='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Spine_dataset.SPINEDetection_test(transform=get_augumentation('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=detection_collate,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_worker,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=None,\n",
    "                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 3, 1408, 512]) torch.Size([1, 17, 5])\n"
     ]
    }
   ],
   "source": [
    "for idx, (images, annotations) in enumerate(train_dataloader):\n",
    "    print(idx ,images.shape, annotations.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "model = EfficientDet(num_classes=num_classes,\n",
    "                     network=network,\n",
    "                     W_bifpn=EFFICIENTDET[network]['W_bifpn'],\n",
    "                     D_bifpn=EFFICIENTDET[network]['D_bifpn'],\n",
    "                     D_class=EFFICIENTDET[network]['D_class'],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(resume is not None):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "device, device_ids = prepare_device(device)\n",
    "model = model.to(device)\n",
    "if(len(device_ids) > 1):\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=3, verbose=True)\n",
    "criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t start training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnav0400/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34057f7369ad4774a524afb8ba5daa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 299.0881941318512\n",
      "    loss           : 2.9873897962411573\n",
      "    bbox_loss      : 0.9788868267422158\n",
      "    cls_loss       : 2.008502986847487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnav0400/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f809a163bac640eb91316b95ad6fff7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 17.27194833755493\n",
      "    loss           : 1.2746460204944015\n",
      "    bbox_loss      : 0.8449379620142281\n",
      "    cls_loss       : 0.4297080524265766\n",
      "1 epoch: \t start training....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34437e0563e1460885f5a4804870e107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 297.42619013786316\n",
      "    loss           : 1.1723392745809098\n",
      "    bbox_loss      : 0.8163026359373715\n",
      "    cls_loss       : 0.3560366389533338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f221d05b16b245d98f413d75dfac29d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 17.23744773864746\n",
      "    loss           : 1.2029424468055367\n",
      "    bbox_loss      : 0.79921450978145\n",
      "    cls_loss       : 0.40372793399728835\n",
      "2 epoch: \t start training....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15880acda4df4841a3d69f0e66d39fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 298.0138063430786\n",
      "    loss           : 0.9662410030741702\n",
      "    bbox_loss      : 0.6977295610984001\n",
      "    cls_loss       : 0.2685114415885257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb166b9b97e49a783fc9741fc06d293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 17.294819593429565\n",
      "    loss           : 0.9495393959805369\n",
      "    bbox_loss      : 0.6804521810263395\n",
      "    cls_loss       : 0.26908721681684256\n",
      "3 epoch: \t start training....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28711f6d6bb14c05bee013dda9001fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 297.6328136920929\n",
      "    loss           : 0.844263854616645\n",
      "    bbox_loss      : 0.6213369534937607\n",
      "    cls_loss       : 0.22292690073563998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0227c724f94791b8c41a122bde1370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 17.218822479248047\n",
      "    loss           : 0.8854044461622834\n",
      "    bbox_loss      : 0.6369846963789314\n",
      "    cls_loss       : 0.24841975304298103\n",
      "4 epoch: \t start training....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5010f04935430499678dd7405bae2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 298.10794162750244\n",
      "    loss           : 0.7724949682824577\n",
      "    bbox_loss      : 0.5744841728909348\n",
      "    cls_loss       : 0.19801079467899338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118e92e98896402e90283a7630a736e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    time           : 17.28127932548523\n",
      "    loss           : 0.7576289779972285\n",
      "    bbox_loss      : 0.567106019007042\n",
      "    cls_loss       : 0.19052295805886388\n",
      "5 epoch: \t start training....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e95be8db4dd4b6b9b7e19c225b0de27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "best_loss = 100\n",
    "df = pd.DataFrame(np.zeros((num_epochs,4)),columns = [\"train_cls\",\"train_bbox_loss\",\"val_cls\",\"val_bbox_loss\"])\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"{} epoch: \\t start training....\".format(epoch))\n",
    "    \n",
    "    start = time.time()\n",
    "    result = {}\n",
    "    total_loss = []\n",
    "    bbox_losses = []\n",
    "    cls_losses = []\n",
    "    corner_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    total_batches = len(train_dataloader)\n",
    "    tk0 = tqdm(train_dataloader, total=total_batches)\n",
    "    for idx, (images, annotations_bboxes) in enumerate(tk0):\n",
    "        images = images.to(device)\n",
    "        annotations_bboxes = annotations_bboxes.to(device)\n",
    "        classification, regression, anchors = model(images)\n",
    "        classification_loss, regression_loss= criterion(\n",
    "            classification, regression, anchors, annotations_bboxes)\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "        loss = classification_loss + regression_loss\n",
    "        if bool(loss == 0):\n",
    "            print('loss equal zero(0)')\n",
    "            continue\n",
    "        loss.backward()\n",
    "        if (idx+1) % grad_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_loss.append(loss.item())\n",
    "        bbox_losses.append(regression_loss.item())\n",
    "        cls_losses.append(classification_loss.item())\n",
    "        tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "    result = {\n",
    "        'time': time.time() - start,\n",
    "        'loss': np.mean(total_loss),\n",
    "        'bbox_loss': np.mean(bbox_losses),\n",
    "        'cls_loss': np.mean(cls_losses)\n",
    "    }\n",
    "    for key, value in result.items():\n",
    "        print('    {:15s}: {}'.format(str(key), value))\n",
    "    df.iloc[epoch,:2] = [np.mean(cls_losses),np.mean(bbox_losses)] \n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        result = {}\n",
    "        total_loss = []\n",
    "        bbox_losses = []\n",
    "        cls_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        total_batches = len(test_dataloader)\n",
    "        tk0 = tqdm(val_dataloader, total=total_batches)\n",
    "        for idx, (images, annotations_bboxes) in enumerate(tk0):\n",
    "            images = images.to(device)\n",
    "            annotations_bboxes = annotations_bboxes.to(device)\n",
    "            classification, regression, anchors = model(images)\n",
    "            classification_loss, regression_loss = criterion(\n",
    "                classification, regression, anchors, annotations_bboxes)\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "            loss = classification_loss + regression_loss\n",
    "            if bool(loss == 0):\n",
    "                print('loss equal zero(0)')\n",
    "                continue\n",
    "            total_loss.append(loss.item())\n",
    "            bbox_losses.append(regression_loss.item())\n",
    "            cls_losses.append(classification_loss.item())\n",
    "            tk0.set_postfix(loss=(np.mean(total_loss)))\n",
    "        result = {\n",
    "            'time': time.time() - start,\n",
    "            'loss': np.mean(total_loss),\n",
    "            'bbox_loss': np.mean(bbox_losses),\n",
    "            'cls_loss': np.mean(cls_losses)\n",
    "        }\n",
    "        for key, value in result.items():\n",
    "            print('    {:15s}: {}'.format(str(key), value))\n",
    "            \n",
    "    scheduler.step(np.mean(total_loss))\n",
    "    df.iloc[epoch,2:] = [np.mean(cls_losses),np.mean(bbox_losses)]\n",
    "    df.to_csv('../logs/d4-downsample-(1536,512)c.csv')\n",
    "    torch.cuda.empty_cache()\n",
    "    arch = type(model).__name__\n",
    "    state = {\n",
    "        'arch': arch,\n",
    "        'num_class': num_classes,\n",
    "        'network': network,\n",
    "        'state_dict': get_state_dict(model)\n",
    "    }\n",
    "    if best_loss>np.mean(bbox_losses):\n",
    "        best_loss = np.mean(bbox_losses)\n",
    "        torch.save(\n",
    "            state, './weights/d4-downsample-(1536,512)c.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(dataloader,split = 'test'):\n",
    "    if split=='test':\n",
    "        for idx, (images) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            image = images[0]\n",
    "            classification_score, category, boxes = model(images)\n",
    "            break\n",
    "    else:\n",
    "        for idx, (images, annotations) in enumerate(dataloader):\n",
    "            image = images[0]\n",
    "            images = images.to(device)\n",
    "            classification_score, category, boxes = model(images)\n",
    "            break\n",
    "    boxes = boxes.detach().cpu().numpy()\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    boxes = boxes.astype(np.int16)\n",
    "    for box in boxes:\n",
    "        start = (box[0],box[1])\n",
    "        end = (box[2],box[3])\n",
    "        image = cv2.rectangle(image, start, end, (255,0,0), 10)\n",
    "    plt.scatter(corners[:,:4],corners[:,4:])\n",
    "    plt.imshow(image.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(val_dataloader,'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(dataloader = val_dataloader):\n",
    "    corners = []\n",
    "    clipped_corners = []\n",
    "    \n",
    "    # Predictions from model\n",
    "    for idx, (image, annotation, corner) in enumerate(val_dataloader):\n",
    "        image = image.to(device)\n",
    "        classification_score, category, pred_boxes, pred_corner = model(image)\n",
    "        classification_score = classification_score.detach().cpu().numpy()\n",
    "        pred_corner = pred_corner.detach().cpu().numpy()\n",
    "        if classification_score.shape[0]>17:\n",
    "            ind = np.argpartition(classification_score, -17)[-17:]\n",
    "            corners.append(pred_corner[ind])\n",
    "        else:\n",
    "            corners.append(pred_corner)\n",
    "    \n",
    "    #Making all predictions 17\n",
    "    for corner in corners:\n",
    "        if corner.shape[0]==17:\n",
    "            clipped_corners.append(corner)\n",
    "        else:\n",
    "            num_repeat = 17-corner.shape[0]\n",
    "            repeat = corner[-1].reshape(1,8)\n",
    "            for i in range(num_repeat):\n",
    "                corner = np.append(corner,repeat,axis=0)\n",
    "            clipped_corners.append(corner)\n",
    "    \n",
    "    clipped_corners = np.array(clipped_corners)\n",
    "    \n",
    "    #Reshaping and saving csv\n",
    "    val_landmarks = np.zeros((len(clipped_corners),136))\n",
    "    for i in range(len(clipped_corners)):\n",
    "        val_landmarks[i,:68] = clipped_corners[i,:,:4].reshape(1,68)/768\n",
    "        val_landmarks[i,68:] = clipped_corners[i,:,4:].reshape(1,68)/1408\n",
    "    \n",
    "    val_preds = pd.DataFrame(val_landmarks)\n",
    "    val_preds.to_csv('val_preds.csv',header = None,index = False)\n",
    "    return val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## find the SMAPE error\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.sum(np.abs(y_true-y_pred), axis=1)\n",
    "    denominator = np.sum(np.abs(y_true+y_pred), axis=1)\n",
    "    smape_val = np.mean(numerator/ denominator) * 100\n",
    "    return smape_val\n",
    "\n",
    "\n",
    "\n",
    "true_csv_path = \"boostnet_labeldata/labels/test/angles.csv\"\n",
    "pred_csv_path = \"boostnet_labeldata/labels/test/test_angles_polyfit.csv\"\n",
    "true_angles = pd.read_csv(true_csv_path, header=None).values\n",
    "pred_angles = pd.read_csv(pred_csv_path, header=None).values\n",
    "print (smape(true_angles, pred_angles))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
